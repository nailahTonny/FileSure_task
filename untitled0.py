# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19dwNe5NOMarNXYibRxF067utl4EhiPQ2
"""

# install dependency
!pip install pymupdf langchain langchain-groq pytesseract pdf2image

!apt-get install poppler-utils  # Needed by pdf2image
!apt-get install tesseract-ocr
!pip install pytesseract pdf2image

# Install dependencies (if not done)
!apt-get install poppler-utils tesseract-ocr -y
!pip install pytesseract pdf2image pillow pymupdf

# upload file
from google.colab import files
uploaded = files.upload()

# Import modules
import fitz  # PyMuPDF
import io
import pytesseract
from pdf2image import convert_from_bytes
from PIL import Image
import json
import re

# ---------- Safe Metadata Extractor ----------
def safe_embfile_info(doc, i):
    try:
        raw_info = doc.embfile_info(i)
        safe_info = {}

        for k, v in raw_info.items():
            if isinstance(v, str):
                # Try strict decode first
                try:
                    safe_info[k] = v.encode('utf-8', errors='strict').decode('utf-8')
                except UnicodeEncodeError:
                    # Fallback to replacing invalid chars
                    safe_info[k] = v.encode('utf-8', errors='replace').decode('utf-8', errors='replace')
            else:
                safe_info[k] = v
        return safe_info

    except Exception as e:
        print(f"[WARN] Metadata extraction failed for attachment {i}: {e}")
        # Try raw object traversal fallback (limited help here)
        return None

# ---------- Extract Text from Embedded PDFs (with OCR fallback) ----------
def extract_text_from_pdf_bytes(pdf_bytes):
    try:
        if not pdf_bytes or len(pdf_bytes) < 100:
            return "[Empty or invalid PDF data]"

        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()

        if not text.strip():
            print("[INFO] No extractable text found — running OCR...")
            images = convert_from_bytes(pdf_bytes)
            ocr_text = ""
            for i, image in enumerate(images):
                ocr_result = pytesseract.image_to_string(image)
                ocr_text += f"\n=== OCR PAGE {i + 1} ===\n{ocr_result}"
            return ocr_text if ocr_text.strip() else "[OCR also found no text]"
        else:
            return text

    except Exception as e:
        return f"[ERROR] Failed to read attached PDF: {e}"

# ---------- Extract Text from Images ----------
def extract_text_from_image_bytes(image_bytes):
    image = Image.open(io.BytesIO(image_bytes))
    text = pytesseract.image_to_string(image)
    return text

# ---------- Extract Text Based on File Type ----------
def extract_text_from_attachment(filename, filedata):
    print(f"[INFO] Extracting: {filename}, size: {len(filedata)} bytes")
    if filename.lower().endswith('.pdf'):
        return extract_text_from_pdf_bytes(filedata)
    elif filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff')):
        return extract_text_from_image_bytes(filedata)
    elif filename.lower().endswith('.txt'):
        try:
            return filedata.decode('utf-8')
        except:
            return "[Error decoding TXT file]"
    else:
        return "[Unsupported attachment type]"

# ---------- Main PDF Processor ----------
def process_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    results = {
        'full_text': "",
        'form_fields': {},
        'attachments': []
    }

    seen_filenames = set()  # To avoid duplicates

    print(f"[INFO] Total embedded files: {doc.embfile_count()}")

    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        results['full_text'] += f"\n=== PAGE {page_num + 1} ===\n"
        results['full_text'] += page.get_text()

        # Form fields
        widgets = page.widgets()
        if widgets:
            for widget in widgets:
                if widget.field_name:
                    results['form_fields'][f"Page {page_num + 1} - {widget.field_name}"] = widget.field_value

        # Attachment annotations
        for annot in page.annots() or []:
            if annot.type[0] == 21:  # FileAttachment
                info = annot.info
                filedata = annot.file_get()
                filename = info.get("title", f"annotation_attachment_page{page_num+1}.pdf")
                if filename not in seen_filenames:
                    print(f"[ANNOT] Attachment found on page {page_num+1}: {filename}")
                    text = extract_text_from_attachment(filename, filedata)
                    results['attachments'].append((filename, text))
                    seen_filenames.add(filename)

    # Embedded files (global attachment tree)
    for i in range(doc.embfile_count()):
        try:
            print(f"[DEBUG] Checking embedded file index {i}...")

            try:
                info = safe_embfile_info(doc, i)
                filename = info.get("filename", f"attachment_{i}.pdf") if info else f"attachment_{i}.pdf"
            except Exception as e:
                print(f"[WARN] Could not read metadata for attachment {i}: {e}")
                filename = f"attachment_{i}.pdf"

            if filename in seen_filenames:
                print(f"[SKIP] Duplicate filename: {filename}")
                continue

            filedata = doc.embfile_get(i)
            print(f"[DEBUG] Extracted embedded file: {filename} ({len(filedata)} bytes)")

            text_content = extract_text_from_attachment(filename, filedata)
            results['attachments'].append((filename, text_content))
            seen_filenames.add(filename)
        except Exception as e:
            print(f"[ERROR] Failed to extract attachment {i}: {e}")


    doc.close()
    return results


# ---------- Run the full pipeline ----------

# Get uploaded file path
pdf_path = next(iter(uploaded))  # First uploaded file

# Run processor
pdf_data = process_pdf(pdf_path)

# ---------- Print Results ----------
print("\n=== FULL TEXT CONTENT ===")
print(pdf_data['full_text'])

print("\n=== FORM FIELD DATA ===")
if pdf_data['form_fields']:
    for field, value in pdf_data['form_fields'].items():
        print(f"{field}: {value}")
else:
    print("No form fields found.")

print("\n=== ATTACHMENTS EXTRACTED ===")
if pdf_data['attachments']:
    for filename, text in pdf_data['attachments']:
        print(f"\nAttachment: {filename}")
        print(f"Extracted Text (first 2000 chars):\n{text[:2000]}...\n")
else:
    print("No attachments extracted.")

# ---------- Extract Specific Fields ----------
def extract_structured_fields(form_fields):
    result = {
        "company_name": "",
        "cin": "",
        "registered_office": "",
        "appointment_date": "",
        "auditor_name": "",
        "auditor_address": "",
        "auditor_frn_or_membership": "",
        "appointment_type": ""
    }

    for key, value in form_fields.items():
        if "CIN_C" in key:
            result["cin"] = value
        elif "CompanyName_C" in key:
            result["company_name"] = value
        elif "CompanyAdd_C" in key:
            result["registered_office"] = value
        elif "DateReceipt_D" in key:
            result["appointment_date"] = value
        elif "NameAuditorFirm_C" in key:
            result["auditor_name"] = value
        elif "permaddress2a_C" in key:
            result["auditor_address"] = value
        elif "permaddress2b_C" in key and result["auditor_address"]:
            result["auditor_address"] += ", " + value
        elif "MemberShNum" in key:
            result["auditor_frn_or_membership"] = value
        elif "DropDownList1" in key:
            result["appointment_type"] = value
    return result

# ---------- Build JSON Output ----------
structured_data = extract_structured_fields(pdf_data['form_fields'])

json_output = {
    **structured_data,
    "attachments": [
        {
            "filename": filename,
            "text_content": text[:3000]  # Save first 3000 chars only
        }
        for filename, text in pdf_data['attachments']
    ]
}

# ---------- Save to JSON ----------
output_file = "adt1_structured_output.json"
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(json_output, f, indent=2, ensure_ascii=False)

print(f"\n✅ Data saved to: {output_file}")

from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage
import json

# Load extracted structured data from JSON
with open("adt1_structured_output.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# ✅ Initialize Groq LLaMA3-8B model with your API key
chat = ChatGroq(
    groq_api_key="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",
    model_name="llama3-8b-8192"
)

# === Generate Summary 1 ===
summary_1_prompt = f"""
Generate a 1-line professional summary in third person for a company auditor appointment disclosure.
Use this data:
- Company Name: {data.get("company_name")}
- Auditor: {data.get("auditor_name")}
- Appointment Date: {data.get("appointment_date")}
- Appointment Type: {data.get("appointment_type")}
Form Filed: ADT-1

Example:
“XYZ Pvt Ltd has appointed M/s Rao & Associates as its statutory auditor for FY 2023–24, effective from 1 July 2023. The appointment has been disclosed via Form ADT-1, with all supporting documents submitted.”
"""

summary_1_response = chat.invoke([HumanMessage(content=summary_1_prompt)])
summary_1 = summary_1_response.content.strip()

# === Generate Summary 2 (per attachment) ===
attachment_text_block = "Summarize each attached letter file in 1–2 lines. Start the summary like:\n" \
                        "- If filename: 'In {filename} file, ...'\n" \
                        "- If no filename: 'In another file, ...'\n\n"

for attachment in data.get("attachments", []):
    filename = attachment.get("filename", "").strip()
    body = attachment.get("text_content", "").strip()
    if not body:
        continue
    label = f"In {filename} file," if filename else "In another file,"
    attachment_text_block += f"{label}\n{body}\n\n"

summary_2_prompt = attachment_text_block + "\nNow summarize each file in a formal tone."
summary_2_response = chat.invoke([HumanMessage(content=summary_2_prompt)])
summary_2 = summary_2_response.content.strip()

# ✅ Create output_text
output_text = f"""=== SUMMARY 1 ===
{summary_1}

=== SUMMARY 2 ===
{summary_2}
"""

# ✅ Save to .txt
with open("adt1_ai_summaries.txt", "w", encoding="utf-8") as f:
    f.write(output_text)

print("✅ AI summaries saved to 'adt1_ai_summaries.txt'")